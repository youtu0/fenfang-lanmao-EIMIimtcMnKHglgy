
以下是一个Python网络爬虫的实践案例，该案例将演示如何使用Python爬取猫眼电影Top100的电影名称、主演和上映时间等信息，并将这些信息保存到TXT文件中。此案例使用了`requests`库来发送HTTP请求，使用`re`库进行正则表达式匹配，并包含详细的代码解释，以确保代码可以直接运行。


## 1\. 准备工作


在开始之前，我们需要确保已经安装了`requests`库。我们可以使用以下命令进行安装：



```
bash复制代码

pip install requests

```

## 2\.猫眼电影Top100的信息是从哪些页面爬取的呢


猫眼电影Top100的信息是从猫眼电影的官方网站（如 [https://maoyan.com/board/4](https://github.com) ）爬取的。具体来说，这个页面展示了猫眼电影Top100的榜单，包含了电影的排名、名称、主演、上映时间、评分等详细信息。


在爬取过程中，爬虫程序会模拟浏览器行为发送HTTP请求到该页面的URL，并接收服务器返回的HTML内容。然后，程序会使用正则表达式或解析库（如BeautifulSoup、lxml等）来解析HTML内容，提取出所需的信息（如电影名称、主演、上映时间等）。


由于猫眼电影的页面结构和反爬虫机制可能会发生变化，因此在实际应用中，爬虫程序可能需要根据实际情况进行调整和优化。此外，爬取网站数据时应遵守相关法律法规和网站的使用协议，不得用于非法用途。


需要注意的是，由于猫眼电影Top100的榜单是动态变化的，因此爬取到的信息可能只是某一时刻的快照。如果需要获取最新或实时的榜单信息，爬虫程序需要定期运行并更新数据。


## 3\. 代码实现


以下是完整的代码示例：



```
import requests
import re
 
# 请求URL
url = 'https://maoyan.com/board/4'
 
# 请求头部，模拟浏览器请求
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36'
}
 
# 解析页面函数
def parse_html(html):
    # 使用正则表达式匹配电影信息
    pattern = re.compile(r'[(.*?)](https://github.com "(.*?)"):[楚门加速器](https://shexiangshi.org)

.*?(.*?)

.*?(.*?)

', re.S)
    items = re.findall(pattern, html)
    
    # 将匹配到的信息转换为字典格式
    for item in items:
        yield {
            '电影名称': item[1],
            '主演': item[2].strip(),
            '上映时间': item[3]
        }
 
# 保存数据函数
def save_data():
    # 打开文件准备写入
    f = open('maoyan_top100.txt', 'w', encoding='utf-8')
    
    # 分页爬取数据，每页10条
    for i in range(10):
        # 构建分页URL
        page_url = f'https://maoyan.com/board/4?offset={i*10}'
        
        # 发送HTTP请求获取页面内容
        response = requests.get(page_url, headers=headers)
        
        # 解析页面内容
        for item in parse_html(response.text):
            # 将信息写入文件
            f.write(str(item) + '\n')
    
    # 关闭文件
    f.close()
 
# 主函数
if __name__ == '__main__':
    save_data()

```

## 4\. 代码解释


* **请求URL和头部**：定义了要爬取的猫眼电影Top100的URL和请求头部，模拟浏览器请求以避免被反爬虫机制拦截。
* **解析页面函数**：`parse_html`函数使用正则表达式匹配页面中的电影信息，包括电影名称、主演和上映时间。正则表达式中的`re.S`标志表示让`.`匹配包括换行符在内的所有字符。
* **保存数据函数**：`save_data`函数负责分页爬取数据，并将解析到的信息写入TXT文件中。通过循环10次，每次构建分页URL并发送请求，然后解析页面内容并写入文件。
* **主函数**：在`__main__`块中调用`save_data`函数开始爬取数据。


## 5\.代码中包含了其他的功能模块


在提供的代码中，虽然主要功能是爬取猫眼电影Top100的信息，但代码结构本身也体现了几个关键的功能模块。这些模块使得代码更加清晰、易于维护和扩展。以下是代码中包含的其他功能模块：


（1）请求发送模块：


* 使用`requests.get`函数发送HTTP GET请求到指定的URL。
* 通过`headers`参数设置请求头部，以模拟浏览器行为。


（2）页面解析模块（parse\_html函数）：


* 使用正则表达式（`re.compile`和`re.findall`）解析HTML内容，提取所需信息。
* 正则表达式定义了要匹配的内容结构，包括电影名称、主演和上映时间等。
* 将匹配到的信息以字典形式返回（通过生成器`yield`逐个返回，节省内存）。


（3）数据保存模块（save\_data函数）：


* 负责将解析到的数据保存到文件中。
* 实现了分页爬取，通过循环构建不同页面的URL并发送请求。
* 将每条电影信息转换为字符串并写入文件，每条信息占一行。


（4）主程序模块（if *name*\=\= *main*:部分）：


* 作为程序的入口点，调用`save_data`函数开始执行爬取任务。
* 确保当该脚本作为主程序运行时才执行爬取操作，而当它被其他脚本导入时不会执行。


（5）错误处理模块（隐含）：


* 虽然代码中没有显式的`try-except`块来处理可能出现的异常（如网络请求失败、解析错误等），但在实际应用中，添加错误处理是非常重要的。
* 可以通过添加异常处理来增强代码的健壮性和用户友好性。


（6）可扩展性模块（隐含）：


* 代码结构清晰，使得添加新功能（如爬取更多信息、支持其他网站等）变得相对容易。
* 可以通过修改正则表达式、添加新的解析函数或数据保存逻辑来扩展代码的功能。


需要注意的是，虽然代码在结构上包含了这些模块，但在实际应用中可能还需要进一步完善，比如添加日志记录、优化正则表达式以提高解析效率、处理动态加载的内容（可能需要使用Selenium等工具）等。此外，由于网站结构和反爬虫机制的变化，代码可能需要根据实际情况进行调整。


## 6\. 运行代码


将上述代码保存为一个Python文件（例如`maoyan_spider.py`），然后在命令行中运行该文件：



```
bash复制代码

python maoyan_spider.py

```

运行完成后，我们会在当前目录下找到一个名为`maoyan_top100.txt`的文件，里面包含了猫眼电影Top100的电影名称、主演和上映时间等信息。


## 7\.注意事项


* 由于网站结构和反爬虫机制可能会发生变化，因此在实际应用中可能需要对代码进行相应的调整。
* 爬取网站数据时应遵守相关法律法规和网站的使用协议，不得用于非法用途。


通过此案例，我们可以学习到如何使用Python进行网络爬虫的基本步骤和方法，包括发送HTTP请求、解析页面内容和保存数据等。希望这个案例对你有所帮助！


